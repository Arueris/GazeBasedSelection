{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "utils.create_unique_folder(\"test\")\n",
    "utils.create_unique_folder(\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_model\n",
    "from TCN import TCNAE\n",
    "\n",
    "path = \"HyperparameterSearchAllModels/Models/TCNAE_472.pth\"\n",
    "paras = {\"seq_len\": 61, \"input_dim\": 1, \"num_channels\": [8, 16], \"kernel_size\": 7, \"latent_dim\": 10}\n",
    "model = load_model(path, TCNAE, paras)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_np_dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "fps = 90\n",
    "cond = \"nod\"\n",
    "f = 62\n",
    "b = 500\n",
    "a = 200\n",
    "angles_correct = np.load(f\"../Data/Dataset_Prepare/angles_fps{fps}_{cond}_Correct_f{f}_b{b}_a{a}.npy\")\n",
    "angles_incorrect = np.load(f\"../Data/Dataset_Prepare/angles_fps{fps}_{cond}_Incorrect_f{f}_b{b}_a{a}.npy\")\n",
    "names_correct = np.load(f\"../Data/Dataset_Prepare/names_fps{fps}_{cond}_Correct_f{f}_b{b}_a{a}.npy\")\n",
    "names_incorrect = np.load(f\"../Data/Dataset_Prepare/names_fps{fps}_{cond}_Incorrect_f{f}_b{b}_a{a}.npy\")\n",
    "pat_names = np.unique(names_correct)\n",
    "n = int(0.7 * len(pat_names))\n",
    "train_pats = pat_names[:n]\n",
    "test_pats = pat_names[n:]\n",
    "train_data = angles_correct[np.isin(names_correct, train_pats)]\n",
    "test_correct = angles_correct[np.isin(names_correct, test_pats)]\n",
    "test_incorrect = angles_incorrect[np.isin(names_incorrect, test_pats)]\n",
    "\n",
    "test_correct = torch.tensor(test_correct).type(torch.float)[:, None, :]\n",
    "test_incorrect = torch.tensor(test_incorrect).type(torch.float)[:, None, :]\n",
    "train_correct= torch.tensor(train_data).type(torch.float)[:, None, :]\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_correct_out = model(train_correct)\n",
    "    correct_out = model(test_correct)\n",
    "    incorrect_out = model(test_incorrect)\n",
    "    mse_correct = torch.mean((correct_out - test_correct)**2, dim=(1, 2))\n",
    "    mse_incorrect = torch.mean((incorrect_out - test_incorrect)**2, dim=(1, 2))\n",
    "    mse_train_correct = torch.mean((train_correct_out - train_correct)**2, dim=(1, 2))\n",
    "    print(mse_correct)\n",
    "    print(mse_incorrect)\n",
    "    print(mse_train_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "perc = np.linspace(80, 100, 100)\n",
    "ths = np.percentile(mse_train_correct.cpu().numpy(), perc)\n",
    "fig, ax = plt.subplots()\n",
    "res = {\"TH_number\": list(), \"TH\": list(), \"Correct\": list(), \"Incorrect\": list()}\n",
    "for p, th in zip(perc, ths):\n",
    "    correct_acc = np.mean((mse_correct < th).cpu().numpy())\n",
    "    incorrect_acc = np.mean((mse_incorrect > th).cpu().numpy())\n",
    "    res[\"TH_number\"].append(th)\n",
    "    res[\"TH\"].append(p)\n",
    "    res[\"Correct\"].append(correct_acc)\n",
    "    res[\"Incorrect\"].append(incorrect_acc)\n",
    "\n",
    "ax.plot(res[\"TH\"], res[\"Correct\"], label=\"Correct\", color=\"b\")\n",
    "ax.plot(res[\"TH\"], res[\"Incorrect\"], label=\"Incorrect\", color=\"orange\")\n",
    "ax.axhline(0.9, linestyle=\"--\", color=\"red\", alpha=0.4)\n",
    "ax.set_ylabel(\"Recall\")\n",
    "ax.set_xlabel(\"Percentile\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_model\n",
    "from TCN import TCNAE\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model_parameter = {\"seq_len\":43, \"input_dim\":1, \"num_channels\": [8, 16], \"kernel_size\":3, \"latent_dim\":10}\n",
    "model = load_model(\"Results/LOSO_500msSeq/Models/TCNAE_12_nod.pth\", TCNAE, model_parameter)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fps = 90\n",
    "cond = \"nod\"\n",
    "f = 44\n",
    "b = 300\n",
    "a = 200\n",
    "angles_correct = np.load(f\"../Data/Dataset_Prepare/angles_fps{fps}_{cond}_Correct_f{f}_b{b}_a{a}.npy\")\n",
    "angles_incorrect = np.load(f\"../Data/Dataset_Prepare/angles_fps{fps}_{cond}_Incorrect_f{f}_b{b}_a{a}.npy\")\n",
    "names_correct = np.load(f\"../Data/Dataset_Prepare/names_fps{fps}_{cond}_Correct_f{f}_b{b}_a{a}.npy\")\n",
    "names_incorrect = np.load(f\"../Data/Dataset_Prepare/names_fps{fps}_{cond}_Incorrect_f{f}_b{b}_a{a}.npy\")\n",
    "pat_names = np.unique(names_correct)\n",
    "n = int(0.7 * len(pat_names))\n",
    "train_pats = pat_names[:n]\n",
    "test_pats = pat_names[n:]\n",
    "train_data = angles_correct[np.isin(names_correct, train_pats)]\n",
    "test_correct = angles_correct[np.isin(names_correct, test_pats)]\n",
    "test_incorrect = angles_incorrect[np.isin(names_incorrect, test_pats)]\n",
    "\n",
    "test_correct = torch.tensor(test_correct).type(torch.float)[:, None, :]\n",
    "test_incorrect = torch.tensor(test_incorrect).type(torch.float)[:, None, :]\n",
    "train_correct= torch.tensor(train_data).type(torch.float)[:, None, :]\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_correct_out = model(train_correct)\n",
    "    correct_out = model(test_correct)\n",
    "    incorrect_out = model(test_incorrect)\n",
    "    mse_correct = torch.mean((correct_out - test_correct)**2, dim=(1, 2))\n",
    "    mse_incorrect = torch.mean((incorrect_out - test_incorrect)**2, dim=(1, 2))\n",
    "    mse_train_correct = torch.mean((train_correct_out - train_correct)**2, dim=(1, 2))\n",
    "    print(mse_correct)\n",
    "    print(mse_incorrect)\n",
    "    print(mse_train_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "trial_nr = 1\n",
    "correct = {\"in\": test_correct[trial_nr][0].numpy(), \"out\": correct_out[trial_nr][0].numpy()}\n",
    "incorrect = {\"in\": test_incorrect[trial_nr][0].numpy(), \"out\": incorrect_out[trial_nr][0].numpy()}\n",
    "\n",
    "fig, (ax_correct, ax_incorrect) = plt.subplots(2, sharex=True)\n",
    "ax_correct.plot(correct[\"in\"], color=\"blue\")\n",
    "ax_correct.set_title(\"Correct\")\n",
    "ax_incorrect.plot(incorrect[\"in\"], color=\"blue\")\n",
    "ax_incorrect.set_title(\"Incorrect\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "mse_correct = np.mean((correct[\"in\"] - correct[\"out\"])**2)\n",
    "mse_incorrect = np.mean((incorrect[\"in\"] - incorrect[\"out\"])**2)\n",
    "\n",
    "fig, (ax_correct, ax_incorrect) = plt.subplots(2, sharex=True)\n",
    "ax_correct.plot(correct[\"out\"], color=\"blue\")\n",
    "ax_correct.set_title(f\"Reconstructed correct\") #; MSE {mse_correct}\")\n",
    "ax_incorrect.plot(incorrect[\"out\"], color=\"blue\")\n",
    "ax_incorrect.set_title(f\"Reconstructed incorrect\") #; MSE {mse_incorrect}\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "print(\"Correct MSE: \", mse_correct)\n",
    "print(\"Incorrect MSE: \", mse_incorrect)\n",
    "# def create_plot_auto(correct, incorrect):\n",
    "#     fig, (ax_correct, ax_incorrect) = plt.subplot(2, sharex=True)\n",
    "#     ax_correct.plot(correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Different algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, ParameterGrid\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_opt_clf(clf, params, correct_samples, incorrect_samples, names_correct, names_incorrect, pats, print_results=True):\n",
    "    kf = KFold(n_splits=5)\n",
    "    best_params = None\n",
    "    best_score = -np.inf\n",
    "    best_results = None\n",
    "    counter_params = np.prod([len(param) for param in params.values()])\n",
    "    \n",
    "    progress_bar = tqdm(total=counter_params, desc=\"Progress\")\n",
    "\n",
    "    for param in ParameterGrid(params):\n",
    "        model = clf(**param)\n",
    "        scores = list()\n",
    "        incorrect_scores = list()\n",
    "        correct_scores = list()\n",
    "\n",
    "        for train_index, test_index in kf.split(pats):\n",
    "            train_pats, test_pats = pats[train_index], pats[test_index]\n",
    "            X_train = correct_samples[np.isin(names_correct, train_pats)]\n",
    "            model.fit(X_train)\n",
    "            X_correct_test = correct_samples[np.isin(names_correct, test_pats)]\n",
    "            X_incorrect_test = incorrect_samples[np.isin(names_incorrect, test_pats)]\n",
    "\n",
    "            correct_score = model.predict(X_correct_test)\n",
    "            incorrect_score = model.predict(X_incorrect_test)\n",
    "            correct_score = (correct_score == 1).mean()\n",
    "            incorrect_score = (incorrect_score == -1).mean()\n",
    "            scores.append((correct_score+incorrect_score)/2)\n",
    "            incorrect_scores.append(incorrect_score)\n",
    "            correct_scores.append(correct_score)\n",
    "        \n",
    "        mean_score = np.mean(scores)\n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score\n",
    "            best_params = param\n",
    "            best_results = {\"correct\": correct_scores, \"incorrect\": incorrect_scores}\n",
    "        \n",
    "        progress_bar.postfix = f\"Best Score: {best_score:.3f}, Correct: {np.mean(best_results['correct']):.3f}, Incorrect: {np.mean(best_results['incorrect']):.3f}\"\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    if print_results:\n",
    "        print()\n",
    "        print(\"CV Results:\")\n",
    "        print(f\"    Best parameters: {best_params}\")\n",
    "        print(f\"    Best score: {best_score}\")\n",
    "        print(f\"    Correct: {np.mean(best_results['correct'])}\")\n",
    "        print(f\"    Incorrect: {np.mean(best_results['incorrect'])}\")\n",
    "        print()\n",
    "\n",
    "    return best_params, best_score, best_results\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall function of method\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def test_method(method, param_grid, fps=90, f=44, b=300, a=200):\n",
    "\n",
    "    all_res = dict()\n",
    "\n",
    "    for cond in [\"gaze\", \"headAndGaze\", \"nod\"]:\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Condition: {cond}\")\n",
    "        print(\"=\"*80)\n",
    "        angles_correct = np.load(f\"../Data/Dataset_Prepare/angles_fps{fps}_{cond}_Correct_f{f}_b{b}_a{a}.npy\")\n",
    "        angles_incorrect = np.load(f\"../Data/Dataset_Prepare/angles_fps{fps}_{cond}_Incorrect_f{f}_b{b}_a{a}.npy\")\n",
    "        names_correct = np.load(f\"../Data/Dataset_Prepare/names_fps{fps}_{cond}_Correct_f{f}_b{b}_a{a}.npy\")\n",
    "        names_incorrect = np.load(f\"../Data/Dataset_Prepare/names_fps{fps}_{cond}_Incorrect_f{f}_b{b}_a{a}.npy\")\n",
    "        pat_names = np.unique(names_correct)\n",
    "        n = int(0.7 * len(pat_names))\n",
    "        train_pats = pat_names[:n]\n",
    "        test_pats = pat_names[n:]\n",
    "        train_data = angles_correct[np.isin(names_correct, train_pats)]\n",
    "        test_correct = angles_correct[np.isin(names_correct, test_pats)]\n",
    "        test_incorrect = angles_incorrect[np.isin(names_incorrect, test_pats)]\n",
    "\n",
    "\n",
    "        # n_estimators: 100, 200, 300 (default: 100)\n",
    "        # max_samples: Number of samples to draw from X to train each base estimator\n",
    "        #       'auto', 0.5, 0.75 (default: 'auto')\n",
    "        # max_features: Number of features to draw from X to train each base estimator\n",
    "        #       0.1, 0.5, 0.75, 1.0 (default: 1.0)\n",
    "        # contamination: Proportion of outliers in the data set\n",
    "        #       'auto', 0.05, 0.1, 0.2 (default: 'auto')\n",
    "\n",
    "\n",
    "        best_params, best_score, best_results = get_opt_clf(method, param_grid, angles_correct, angles_incorrect, names_correct, names_incorrect, train_pats)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        clf = method(**best_params).fit(train_data)\n",
    "\n",
    "        # clf = IsolationForest(max_features=1.0, random_state=42).fit(train_data)\n",
    "\n",
    "        # model = IsolationForest()\n",
    "        # param_grid = {\n",
    "        #     \"n_estimators\": [100, 200, 300],\n",
    "        #     \"max_samples\": ['auto', 0.5, 0.75],\n",
    "        #     \"max_features\": [0.1, 0.5, 0.75, 1.0],\n",
    "        #     \"contamination\": ['auto', 0.05, 0.1, 0.2]   \n",
    "        # }\n",
    "\n",
    "        # clf = GridSearchCV(model, param_grid)\n",
    "\n",
    "\n",
    "        correct_scores = clf.predict(test_correct)\n",
    "        incorrect_scores = clf.predict(test_incorrect)\n",
    "\n",
    "        correct_acc = (correct_scores==1).mean()\n",
    "        incorrect_acc = (incorrect_scores==-1).mean()\n",
    "\n",
    "        print(\"Correct\", (correct_scores==1).mean(), \"Incorrect\", (incorrect_scores==-1).mean())\n",
    "\n",
    "        sns.histplot({\"Correct\": correct_scores, \"Incorrect\": incorrect_scores},\n",
    "                    multiple=\"layer\", common_norm=False, stat=\"percent\")\n",
    "        plt.show()\n",
    "\n",
    "        all_res[cond] = {\n",
    "            \"best_params\": best_params,\n",
    "            \"best_score\": best_score,\n",
    "            \"best_results\": best_results,\n",
    "            \"correct_test\": correct_acc,\n",
    "            \"incorrect_test\": incorrect_acc,\n",
    "            \"macro_test\": (correct_acc + incorrect_acc) / 2,\n",
    "            \"micro_test\": (correct_acc * len(correct_scores) + incorrect_acc * len(incorrect_scores)) / (len(correct_scores) + len(incorrect_scores))\n",
    "        }\n",
    "\n",
    "        print()\n",
    "\n",
    "    with open(f\"Results/ml_algos_res/{method.__name__}.json\", \"w\") as f:\n",
    "        json.dump(all_res, f, indent=4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "param_grid = {\n",
    "        \"n_estimators\": [100, 200, 300],\n",
    "        \"max_samples\": ['auto', 0.5, 0.75],\n",
    "        \"max_features\": [0.1, 0.5, 0.75, 1.0],\n",
    "        \"contamination\": ['auto', 0.05, 0.1, 0.2]   \n",
    "    }\n",
    "\n",
    "test_method(IsolationForest, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import IsolationForest\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from utils import load_np_dataset\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# import numpy as np\n",
    "# fps = 90\n",
    "# cond = \"nod\"\n",
    "# f = 44\n",
    "# b = 300\n",
    "# a = 200\n",
    "# all_res = dict()\n",
    "\n",
    "# for cond in [\"gaze\", \"headAndGaze\", \"nod\"]:\n",
    "#     print(\"=\"*80)\n",
    "#     print(f\"Condition: {cond}\")\n",
    "#     print(\"=\"*80)\n",
    "#     angles_correct = np.load(f\"../Data/Dataset_Prepare/angles_fps{fps}_{cond}_Correct_f{f}_b{b}_a{a}.npy\")\n",
    "#     angles_incorrect = np.load(f\"../Data/Dataset_Prepare/angles_fps{fps}_{cond}_Incorrect_f{f}_b{b}_a{a}.npy\")\n",
    "#     names_correct = np.load(f\"../Data/Dataset_Prepare/names_fps{fps}_{cond}_Correct_f{f}_b{b}_a{a}.npy\")\n",
    "#     names_incorrect = np.load(f\"../Data/Dataset_Prepare/names_fps{fps}_{cond}_Incorrect_f{f}_b{b}_a{a}.npy\")\n",
    "#     pat_names = np.unique(names_correct)\n",
    "#     n = int(0.7 * len(pat_names))\n",
    "#     train_pats = pat_names[:n]\n",
    "#     test_pats = pat_names[n:]\n",
    "#     train_data = angles_correct[np.isin(names_correct, train_pats)]\n",
    "#     test_correct = angles_correct[np.isin(names_correct, test_pats)]\n",
    "#     test_incorrect = angles_incorrect[np.isin(names_incorrect, test_pats)]\n",
    "\n",
    "\n",
    "#     # n_estimators: 100, 200, 300 (default: 100)\n",
    "#     # max_samples: Number of samples to draw from X to train each base estimator\n",
    "#     #       'auto', 0.5, 0.75 (default: 'auto')\n",
    "#     # max_features: Number of features to draw from X to train each base estimator\n",
    "#     #       0.1, 0.5, 0.75, 1.0 (default: 1.0)\n",
    "#     # contamination: Proportion of outliers in the data set\n",
    "#     #       'auto', 0.05, 0.1, 0.2 (default: 'auto')\n",
    "\n",
    "#     param_grid = {\n",
    "#         \"n_estimators\": [100, 200, 300],\n",
    "#         \"max_samples\": ['auto', 0.5, 0.75],\n",
    "#         \"max_features\": [0.1, 0.5, 0.75, 1.0],\n",
    "#         \"contamination\": ['auto', 0.05, 0.1, 0.2]   \n",
    "#     }\n",
    "\n",
    "#     for k in param_grid.keys():\n",
    "#         all_res[k] = list()\n",
    "\n",
    "#     best_params, best_score, best_results = get_opt_clf(IsolationForest, param_grid, angles_correct, angles_incorrect, names_correct, names_incorrect, train_pats)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     clf = IsolationForest(**best_params).fit(train_data)\n",
    "\n",
    "#     # clf = IsolationForest(max_features=1.0, random_state=42).fit(train_data)\n",
    "\n",
    "#     # model = IsolationForest()\n",
    "#     # param_grid = {\n",
    "#     #     \"n_estimators\": [100, 200, 300],\n",
    "#     #     \"max_samples\": ['auto', 0.5, 0.75],\n",
    "#     #     \"max_features\": [0.1, 0.5, 0.75, 1.0],\n",
    "#     #     \"contamination\": ['auto', 0.05, 0.1, 0.2]   \n",
    "#     # }\n",
    "\n",
    "#     # clf = GridSearchCV(model, param_grid)\n",
    "\n",
    "\n",
    "#     correct_scores = clf.predict(test_correct)\n",
    "#     incorrect_scores = clf.predict(test_incorrect)\n",
    "\n",
    "#     correct_acc = (correct_scores==1).mean()\n",
    "#     incorrect_acc = (incorrect_scores==-1).mean()\n",
    "\n",
    "#     print(\"Correct\", (correct_scores==1).mean(), \"Incorrect\", (incorrect_scores==-1).mean())\n",
    "\n",
    "#     sns.histplot({\"Correct\": correct_scores, \"Incorrect\": incorrect_scores},\n",
    "#                 multiple=\"layer\", common_norm=False, stat=\"percent\")\n",
    "#     plt.show()\n",
    "\n",
    "#     all_res[cond] = {\n",
    "#         \"best_params\": best_params,\n",
    "#         \"best_score\": best_score,\n",
    "#         \"best_results\": best_results,\n",
    "#         \"correct_test\": correct_acc,\n",
    "#         \"incorrect_test\": incorrect_acc,\n",
    "#         \"macro_test\": (correct_acc + incorrect_acc) / 2,\n",
    "#         \"micro_test\": (correct_acc * len(correct_scores) + incorrect_acc * len(incorrect_scores)) / (len(correct_scores) + len(incorrect_scores))\n",
    "#     }\n",
    "\n",
    "#     print()\n",
    "\n",
    "# import json\n",
    "# with open(\"Results/ml_algos_res/IsolationForest.json\", \"w\") as f:\n",
    "#     json.dump(all_res, f, indent=4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "param_grid = {\n",
    "        \"nu\": [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "        \"kernel\": [\"linear\", \"rbf\", \"poly\", \"sigmoid\"],\n",
    "        \"gamma\": ['scale', 'auto']\n",
    "    }\n",
    "\n",
    "test_method(OneClassSVM, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "\n",
    "fps = 90\n",
    "cond = \"gaze\"\n",
    "f = 44\n",
    "b = 300\n",
    "a = 200\n",
    "\n",
    "for cond in [\"gaze\", \"headAndGaze\", \"nod\"]:#\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Condition: {cond}\")\n",
    "    print(\"=\"*80)\n",
    "    angles_correct = np.load(f\"../Data/Dataset_Prepare/angles_fps{fps}_{cond}_Correct_f{f}_b{b}_a{a}.npy\")\n",
    "    angles_incorrect = np.load(f\"../Data/Dataset_Prepare/angles_fps{fps}_{cond}_Incorrect_f{f}_b{b}_a{a}.npy\")\n",
    "    names_correct = np.load(f\"../Data/Dataset_Prepare/names_fps{fps}_{cond}_Correct_f{f}_b{b}_a{a}.npy\")\n",
    "    names_incorrect = np.load(f\"../Data/Dataset_Prepare/names_fps{fps}_{cond}_Incorrect_f{f}_b{b}_a{a}.npy\")\n",
    "    pat_names = np.unique(names_correct)\n",
    "    n = int(0.7 * len(pat_names))\n",
    "    train_pats = pat_names[:n]\n",
    "    test_pats = pat_names[n:]\n",
    "    train_data = angles_correct[np.isin(names_correct, train_pats)]\n",
    "    test_correct = angles_correct[np.isin(names_correct, test_pats)]\n",
    "    test_incorrect = angles_incorrect[np.isin(names_incorrect, test_pats)]\n",
    "\n",
    "    # kernel: \"linear\", \"rbf\", \"poly\", \"sigmoid\", \"precomputed\"\n",
    "    # nu: An upper bound on the fraction of training errors and a lower bound of the fraction of support vectors. \n",
    "    #     Should be in the interval (0, 1]. By default 0.5 will be taken.\n",
    "    # gamma: gamma is a hyperparameter that we have to set before the training model.\n",
    "    #        - 'scale': uses 1 / (n_features * X.var()) as value of gamma\n",
    "    #        - 'auto': uses 1 / n_features.\n",
    "    #        - float > 0\n",
    "    param_grid = {\n",
    "        \"nu\": [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "        \"kernel\": [\"linear\", \"rbf\", \"poly\", \"sigmoid\"],\n",
    "        \"gamma\": ['scale', 'auto']\n",
    "    }\n",
    "\n",
    "    best_params, best_score, best_results = get_opt_clf(svm.OneClassSVM, param_grid, angles_correct, angles_incorrect, names_correct, names_incorrect, train_pats)\n",
    "\n",
    "\n",
    "    clf = svm.OneClassSVM(**best_params)\n",
    "    clf.fit(train_data)\n",
    "\n",
    "    correct_scores = clf.predict(test_correct)\n",
    "    incorrect_scores = clf.predict(test_incorrect)\n",
    "\n",
    "    print(\"Correct\", (correct_scores==1).mean(), \"Incorrect\", (incorrect_scores==-1).mean())\n",
    "\n",
    "    sns.histplot({\"Correct\": correct_scores, \"Incorrect\": incorrect_scores},\n",
    "                multiple=\"layer\", common_norm=False, stat=\"percent\")\n",
    "    plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local outliere factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "fps = 90\n",
    "cond = \"gaze\"\n",
    "f = 44\n",
    "b = 300\n",
    "a = 200\n",
    "\n",
    "for gaze in [\"gaze\", \"headAndGaze\", \"nod\"]:\n",
    "    angles_correct = np.load(f\"../Data/Dataset_Prepare/angles_fps{fps}_{cond}_Correct_f{f}_b{b}_a{a}.npy\")\n",
    "    angles_incorrect = np.load(f\"../Data/Dataset_Prepare/angles_fps{fps}_{cond}_Incorrect_f{f}_b{b}_a{a}.npy\")\n",
    "    names_correct = np.load(f\"../Data/Dataset_Prepare/names_fps{fps}_{cond}_Correct_f{f}_b{b}_a{a}.npy\")\n",
    "    names_incorrect = np.load(f\"../Data/Dataset_Prepare/names_fps{fps}_{cond}_Incorrect_f{f}_b{b}_a{a}.npy\")\n",
    "    pat_names = np.unique(names_correct)\n",
    "    n = int(0.7 * len(pat_names))\n",
    "    train_pats = pat_names[:n]\n",
    "    test_pats = pat_names[n:]\n",
    "    train_data = angles_correct[np.isin(names_correct, train_pats)]\n",
    "    test_correct = angles_correct[np.isin(names_correct, test_pats)]\n",
    "    test_incorrect = angles_incorrect[np.isin(names_incorrect, test_pats)]\n",
    "\n",
    "    # n_neighbors: Number of neighbors to use by default for kneighbors queries.\n",
    "    #      5, 10, 20, 50 (default: 20)\n",
    "    # algorithm: Algorithm used to compute the nearest neighbors\n",
    "    #     'auto', 'ball_tree', 'kd_tree', 'brute' (default: 'auto')\n",
    "    # leaf_size: Leaf size passed to BallTree or KDTree.\n",
    "    #     10, 20, 30, 40, 50 (default: 30)\n",
    "    # metric: metric used for the distance computation\n",
    "    #     'euclidean', 'manhattan', 'minkowski', 'chebyshev' (default: 'minkowski')\n",
    "\n",
    "    param_grid = {\n",
    "        \"n_neighbors\": [5, 10, 20, 50],\n",
    "        \"algorithm\": ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        \"leaf_size\": [10, 20, 30, 40, 50],\n",
    "        \"metric\": ['euclidean', 'manhattan', 'minkowski', 'chebyshev'],\n",
    "        'novelty': [True]\n",
    "    }\n",
    "\n",
    "    best_params, best_score, best_results = get_opt_clf(LocalOutlierFactor, param_grid, angles_correct, angles_incorrect, names_correct, names_incorrect, train_pats)\n",
    "\n",
    "\n",
    "    clf = LocalOutlierFactor(**best_params)\n",
    "    clf.fit(train_data)\n",
    "\n",
    "    correct_scores = clf.predict(test_correct)\n",
    "    incorrect_scores = clf.predict(test_incorrect)\n",
    "\n",
    "    print(\"Correct\", (correct_scores==1).mean(), \"Incorrect\", (incorrect_scores==-1).mean())\n",
    "\n",
    "    sns.histplot({\"Correct\": correct_scores, \"Incorrect\": incorrect_scores},\n",
    "                multiple=\"layer\", common_norm=False, stat=\"percent\")\n",
    "    plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elliptic Envolpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "fps = 90\n",
    "cond = \"gaze\"\n",
    "f = 44\n",
    "b = 300\n",
    "a = 200\n",
    "angles_correct = np.load(f\"../Data/Dataset_Prepare/angles_fps{fps}_{cond}_Correct_f{f}_b{b}_a{a}.npy\")\n",
    "angles_incorrect = np.load(f\"../Data/Dataset_Prepare/angles_fps{fps}_{cond}_Incorrect_f{f}_b{b}_a{a}.npy\")\n",
    "names_correct = np.load(f\"../Data/Dataset_Prepare/names_fps{fps}_{cond}_Correct_f{f}_b{b}_a{a}.npy\")\n",
    "names_incorrect = np.load(f\"../Data/Dataset_Prepare/names_fps{fps}_{cond}_Incorrect_f{f}_b{b}_a{a}.npy\")\n",
    "pat_names = np.unique(names_correct)\n",
    "n = int(0.7 * len(pat_names))\n",
    "train_pats = pat_names[:n]\n",
    "test_pats = pat_names[n:]\n",
    "train_data = angles_correct[np.isin(names_correct, train_pats)]\n",
    "test_correct = angles_correct[np.isin(names_correct, test_pats)]\n",
    "test_incorrect = angles_incorrect[np.isin(names_incorrect, test_pats)]\n",
    "\n",
    "# support_fraction: The proportion of points to be included in the support of the raw MCD estimate.\n",
    "#      if None, the minimum value of support_fraction will be used within the algorithm: [n_samples + n_features + 1] / 2\n",
    "#      0.1, 0.2, 0.3, 0.4, 0.5 (default: None), Range: (0, 1)\n",
    "# contamination: The amount of contamination of the data set, i.e. the proportion of outliers in the data set.\n",
    "#      0.05, 0.1, 0.2 (default: 0.1), Range: (0, 0.5)\n",
    "\n",
    "clf = EllipticEnvelope(contamination=0.1)\n",
    "clf.fit(train_data)\n",
    "\n",
    "correct_scores = clf.predict(test_correct)\n",
    "incorrect_scores = clf.predict(test_incorrect)\n",
    "\n",
    "print(\"Correct\", (correct_scores==1).mean(), \"Incorrect\", (incorrect_scores==-1).mean())\n",
    "\n",
    "sns.histplot({\"Correct\": correct_scores, \"Incorrect\": incorrect_scores},\n",
    "             multiple=\"layer\", common_norm=False, stat=\"percent\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "fps = 90\n",
    "cond = \"gaze\"\n",
    "f = 44\n",
    "b = 300\n",
    "a = 200\n",
    "angles_correct = np.load(f\"../Data/Dataset_Prepare/angles_fps{fps}_{cond}_Correct_f{f}_b{b}_a{a}.npy\")\n",
    "angles_incorrect = np.load(f\"../Data/Dataset_Prepare/angles_fps{fps}_{cond}_Incorrect_f{f}_b{b}_a{a}.npy\")\n",
    "names_correct = np.load(f\"../Data/Dataset_Prepare/names_fps{fps}_{cond}_Correct_f{f}_b{b}_a{a}.npy\")\n",
    "names_incorrect = np.load(f\"../Data/Dataset_Prepare/names_fps{fps}_{cond}_Incorrect_f{f}_b{b}_a{a}.npy\")\n",
    "pat_names = np.unique(names_correct)\n",
    "n = int(0.7 * len(pat_names))\n",
    "train_pats = pat_names[:n]\n",
    "test_pats = pat_names[n:]\n",
    "train_data = angles_correct[np.isin(names_correct, train_pats)]\n",
    "test_correct = angles_correct[np.isin(names_correct, test_pats)]\n",
    "test_incorrect = angles_incorrect[np.isin(names_incorrect, test_pats)]\n",
    "\n",
    "clf = DBSCAN(eps=3, min_samples=10)\n",
    "# clf.fit(train_data)\n",
    "\n",
    "correct_scores = clf.fit_predict(test_correct)\n",
    "incorrect_scores = clf.fit_predict(test_incorrect)\n",
    "\n",
    "print(\"Correct\", (correct_scores==1).mean(), \"Incorrect\", (incorrect_scores==-1).mean())\n",
    "\n",
    "sns.histplot({\"Correct\": correct_scores, \"Incorrect\": incorrect_scores},\n",
    "             multiple=\"layer\", common_norm=False, stat=\"percent\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correct\", len(angles_correct)/len(pat_names), \"Incorrect\", len(angles_incorrect)/len(pat_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch.nn as nn\n",
    "\n",
    "test_dict = {\n",
    "    \"name\": \"bla\",\n",
    "    \"res\": 0.890709001,\n",
    "    \"paras\": {\n",
    "        \"num_channels\": [8, 16],\n",
    "        \"kernel_size\": 5,\n",
    "        \"latent_dim\": 16\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"num_epochs\": 100,\n",
    "        \"batch_size\": 1000,\n",
    "        \"criterion\": str(nn.MSELoss())\n",
    "    },\n",
    "    \"th_perc\": 95,\n",
    "    \"th_value\": 0.335\n",
    "}\n",
    "\n",
    "with open(\"example.json\", \"w\") as outfile:\n",
    "    json.dump(test_dict, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"example.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    print(type(data))\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = [[int, {'x': 1, 'y': 2}], [int, {'x': 1, 'y': 2}], [int, {'x': 1, 'y': 2}]]\n",
    "\n",
    "te = [x + [y] for y in [95, 99] for x in te]\n",
    "te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from TCN import TCNAE\n",
    "import utils\n",
    "import json\n",
    "import torch.onnx\n",
    "\n",
    "path = \"Results/LOSO_a200_b300_f44_seq43/Models\"\n",
    "model_name = \"TCNAE_4\"\n",
    "conditions = [\"nod\", \"gaze\", \"headAndGaze\"]\n",
    "model_names = [\"TCNAE_1_gaze\", \"TCNAE_1_headAndGaze\", \"TCNAE_1_nod\"]\n",
    "cond = conditions[0]\n",
    "# for cond in conditions:\n",
    "#     with open(os.path.join(path, model_name + f\"_{cond}_info.json\")) as json_file:\n",
    "#         model_info = json.load(json_file)\n",
    "        # print(model_info)\n",
    "    # model = utils.load_model(os.path.join(path, model_name + f\"_{cond}.pth\"), TCNAE, model_info[\"model_parameter\"])\n",
    "\n",
    "for model_name in model_names:\n",
    "    with open(os.path.join(path, model_name + \"_info.json\")) as json_file:\n",
    "        model_info = json.load(json_file)\n",
    "\n",
    "    model = utils.load_model(os.path.join(path, model_name + \".pth\"), TCNAE, model_info[\"model_parameter\"])\n",
    "\n",
    "    dummy_input = torch.randn(1, 1, model_info[\"model_parameter\"][\"seq_len\"])\n",
    "    model.eval()\n",
    "    # dummy_output = model(dummy_input)\n",
    "    # print(dummy_output)\n",
    "\n",
    "    # save model as onnx at the same location like the model_info\n",
    "\n",
    "    torch.onnx.export(model, dummy_input, os.path.join(path, model_name + \".onnx\"))\n",
    "\n",
    "    print(f\"Model has been saved as {model_name}.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test models\n",
    "input = torch.randn(5, 1, 1, 1, 1, 1, 1, 61)\n",
    "output = model(input)\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAIA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
